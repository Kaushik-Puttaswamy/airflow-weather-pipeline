# ğŸŒ¦ï¸ Weather Data ETL Pipeline with Airflow, PostgreSQL & Docker

This project demonstrates a complete **ETL (Extract, Transform, Load)** pipeline using [Open-Meteo](https://open-meteo.com/) weather API. The pipeline is orchestrated using **Apache Airflow**, loads the data into **PostgreSQL (via Docker)**, and can be explored using **DBeaver**.

---

## ğŸ“Œ Project Architecture

The following architecture illustrates the end-to-end workflow from data extraction to loading and orchestration:

![Project Architecture](Project%20architecture.png)

---

## ğŸ”— API & Database Connections

Airflow uses built-in **Connections** for both the HTTP API and PostgreSQL database. Below are the screenshots of how those are configured:

- âœ… Open Meteo API Connection:
  
  ![Open Meteo API](Open-meteo-API-connection.png)

- âœ… PostgreSQL Connection:

  ![PostgreSQL Connection](postgresql_connection.png)

- âœ… Full Connection List in Airflow UI:

  ![Connection Lists](Connection-lists.png)

---

## âš™ï¸ Tools Used

- **Airflow** for orchestration
- **PostgreSQL** for data storage (via Docker)
- **Open-Meteo API** for fetching weather data
- **DBeaver** for visualizing the data
- **Docker Compose** for environment setup

---

## ğŸ—‚ï¸ Project Structure

â”œâ”€â”€ dags/                   # Custom Airflow DAGs (ETL, examples)

â”‚Â Â  â”œâ”€â”€ etlweather.py       # Weather ETL pipeline (this project)

â”‚Â Â  â””â”€â”€ exampledag.py       # Example DAG from Astronomer

â”œâ”€â”€ tests/dags/             # Unit + integration tests

â”œâ”€â”€ docker-compose.yml      # Postgres service

â”œâ”€â”€ Dockerfile              # Custom Airflow image (extends astroâ€‘runtime)

â”œâ”€â”€ airflow_settings.yaml   # Preâ€‘seeded connections & variables

â”œâ”€â”€ requirements.txt        # Extra Python deps (if any)

â”œâ”€â”€ packages.txt            # OS packages to install into image

â””â”€â”€ README.md               # â† you are here

---

## ğŸ”„ DAG Workflow

The following DAG orchestrates the ETL process:

1. **Extract** weather data from Open-Meteo API

2. **Transform** relevant fields

3. **Load** the processed data into PostgreSQL

![DAG Workflow](Dag_workflow.png)

---

## ğŸ“Š Loaded Data View (in DBeaver)

You can connect DBeaver to the local PostgreSQL database (`localhost:5432`) to explore the data:

![Loaded Data](Loaded_data.png)

---

## ğŸ§± Weather Data Table Schema

Hereâ€™s the schema for the `weather_data` table in PostgreSQL:

```sql
CREATE TABLE IF NOT EXISTS weather_data (
    latitude FLOAT,
    longitude FLOAT,
    temperature FLOAT,
    windspeed FLOAT,
    winddirection FLOAT,
    weathercode INT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```
## ğŸ³ Docker Setup

You can bring up the environment using Docker:
# Build and start services
astro dev start

Ensure Docker is running, and check logs if Airflow fails to start:

``` 
astro dev logs --webserver
astro dev logs --scheduler
```

## ğŸ› ï¸ Airflow UI Access

Once started, access the Airflow UI at:

``` http://localhost:8080 ```

Use default credentials:
	
 â€¢	Username: admin
	
 â€¢	Password: admin (or whatever youâ€™ve set)

## ğŸ§ª Testing & Validation

â€¢	You can use DBeaver or psql CLI to confirm that data is being inserted into the weather_data table.

â€¢	Modify the DAG or add alerts if needed.

## âœ… Conclusion

This project showcases how to build a scalable, automated, and reproducible ETL pipeline using modern tools. Youâ€™ve set up data extraction, transformation, loading into a database, and built a robust Airflow DAG for scheduling.

## ğŸ“ Appendix

â€¢	Open-Meteo API: https://open-meteo.com/

â€¢	Airflow Docs: https://airflow.apache.org/docs/

â€¢	Astronomer CLI: https://docs.astronomer.io/astro/cli-install

â€¢	DBeaver: https://dbeaver.io/

---

Let me know if you want this saved as a file or want to auto-generate it into a project!
